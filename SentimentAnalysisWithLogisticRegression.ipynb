{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first project is to explore logistic regression and feature engineering with existing Turi Create functions.\n",
    "\n",
    "In this project we will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "\n",
    "* Use SFrames to do some feature engineering\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the logistic regression weights, predictors and ground truth labels, write a function to compute the **accuracy** of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "    \n",
    "## Fire up Turi Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import turicreate as tc\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = tc.SFrame(\"amazon_baby.sframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = text.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text\n",
    "\n",
    "review_without_punctuation = products['review'].apply(remove_punctuation)\n",
    "products['word_count'] = tc.text_analytics.count_words(review_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = products.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = tc.logistic_classifier.create(train_data,\n",
    "                                                        target = 'sentiment',\n",
    "                                                        features=['word_count'],\n",
    "                                                        validation_set=None,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91073\n"
     ]
    }
   ],
   "source": [
    "weights = sentiment_model.coefficients\n",
    "num_positive_weights = len(weights[weights['value'] >= 0])\n",
    "print(num_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -1, -1] [1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "sample_test_data = test_data[10:13]\n",
    "scores = sentiment_model.predict(sample_test_data, output_type='margin')\n",
    "l = []\n",
    "for i in scores:\n",
    "    if i > 0:\n",
    "        l.append(1)\n",
    "    else:\n",
    "        l.append(-1)\n",
    "print(l,sentiment_model.predict(sample_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9917471313286872, 0.047390547487155835, 0.00027775277121725486]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = []\n",
    "for i in scores:\n",
    "    p = 1/(1+ math.exp(-i))\n",
    "    prob.append(p)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+--------+\n",
      "|              name             |             review            | rating |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "| Fisher-Price Cradle 'N Swi... | My husband and I cannot st... |  5.0   |\n",
      "| The Original CJ's BuTTer (... | I'm going to try to review... |  4.0   |\n",
      "| Baby Jogger City Mini GT D... | We are well pleased with t... |  4.0   |\n",
      "| Diono RadianRXT Convertibl... | Like so many others before... |  5.0   |\n",
      "| Diono RadianRXT Convertibl... | I bought this seat for my ... |  5.0   |\n",
      "| Graco Pack 'n Play Element... | My husband and I assembled... |  4.0   |\n",
      "| Maxi-Cosi Pria 70 with Tin... | We love this car seat!! It... |  5.0   |\n",
      "| Britax 2012 B-Agile Stroll... | [I got this stroller for m... |  4.0   |\n",
      "| Quinny 2012 Buzz Stroller,... | Choice - Quinny Buzz 2011 ... |  4.0   |\n",
      "| Roan Rocco Classic Pram St... | Great Pram Rocco!!!!!!I bo... |  5.0   |\n",
      "| Britax Decathlon Convertib... | I researched a few differe... |  4.0   |\n",
      "| bumGenius One-Size Snap Cl... | Warning: long review!Short... |  5.0   |\n",
      "| Infantino Wrap and Tie Bab... | I bought this carrier when... |  5.0   |\n",
      "| Baby Einstein Around The W... | I am so HAPPY I brought th... |  5.0   |\n",
      "| Britax Frontier Booster Ca... | My family previously owned... |  5.0   |\n",
      "| Evenflo X Sport Plus Conve... | After seeing this in Paren... |  5.0   |\n",
      "| P'Kolino Silly Soft Seatin... | I've purchased both the P'... |  4.0   |\n",
      "| Peg Perego Aria Light Weig... | We have 3 strollers...one ... |  5.0   |\n",
      "| Fisher-Price Rainforest Me... | My daughter wasn't able to... |  5.0   |\n",
      "| Lilly Gold Sit 'n' Stroll ... | I just completed a two-mon... |  5.0   |\n",
      "+-------------------------------+-------------------------------+--------+\n",
      "+-------------------------------+-----------+-------------+\n",
      "|           word_count          | sentiment | probability |\n",
      "+-------------------------------+-----------+-------------+\n",
      "| {'recommendations': 1.0, '... |     1     |     1.0     |\n",
      "| {'order': 1.0, 'latest': 1... |     1     |     1.0     |\n",
      "| {'buy': 1.0, 'deal': 1.0, ... |     1     |     1.0     |\n",
      "| {'stroller': 1.0, 'traveli... |     1     |     1.0     |\n",
      "| {'really': 1.0, 'needs': 1... |     1     |     1.0     |\n",
      "| {'works': 1.0, 'sleeping':... |     1     |     1.0     |\n",
      "| {'well': 1.0, 'knowing': 1... |     1     |     1.0     |\n",
      "| {'allaround': 1.0, 'bill':... |     1     |     1.0     |\n",
      "| {'shop': 1.0, 'lucked': 1.... |     1     |     1.0     |\n",
      "| {'sell': 1.0, 'regret': 1.... |     1     |     1.0     |\n",
      "| {'enough': 1.0, 'big': 1.0... |     1     |     1.0     |\n",
      "| {'section': 1.0, 'comment'... |     1     |     1.0     |\n",
      "| {'well': 1.0, 'mouth': 1.0... |     1     |     1.0     |\n",
      "| {'go': 1.0, 'steals': 1.0,... |     1     |     1.0     |\n",
      "| {'looks': 1.0, 'now': 1.0,... |     1     |     1.0     |\n",
      "| {'compliments': 1.0, 'gett... |     1     |     1.0     |\n",
      "| {'admit': 1.0, 'though': 1... |     1     |     1.0     |\n",
      "| {'collapses': 1.0, 'awesom... |     1     |     1.0     |\n",
      "| {'youll': 1.0, 'afford': 1... |     1     |     1.0     |\n",
      "| {'toddler': 1.0, 'or': 1.0... |     1     |     1.0     |\n",
      "+-------------------------------+-----------+-------------+\n",
      "[20 rows x 6 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prob = sentiment_model.predict(test_data, output_type='probability')\n",
    "test_data['probability'] = test_prob\n",
    "test_data_sorted = test_data.topk('probability', 20)\n",
    "test_data_sorted.print_rows(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    # First get the predictions\n",
    "    predictions = model.predict(data)\n",
    "    \n",
    "    # Compute the number of correctly classified examples\n",
    "    ctr = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == true_labels[i]:\n",
    "            ctr += 1\n",
    "\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    accuracy = ctr / len(predictions)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9221862251019919"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1 = get_classification_accuracy(sentiment_model, test_data, test_data['sentiment'])\n",
    "acc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic model with significant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------+----------------------+\n",
      "|        name       |    index     | class |        value         |\n",
      "+-------------------+--------------+-------+----------------------+\n",
      "| word_count_subset |    loves     |   1   |  1.6772714555592905  |\n",
      "| word_count_subset |   perfect    |   1   |  1.5144862670271344  |\n",
      "| word_count_subset |     love     |   1   |  1.3654354936790376  |\n",
      "|    (intercept)    |     None     |   1   |  1.2995449552027034  |\n",
      "| word_count_subset |     easy     |   1   |  1.1936618983284653  |\n",
      "| word_count_subset |    great     |   1   |  0.9446912694798453  |\n",
      "| word_count_subset |    little    |   1   |  0.5206286360250189  |\n",
      "| word_count_subset |     well     |   1   |  0.5042567463979287  |\n",
      "| word_count_subset |     able     |   1   | 0.19143830229475103  |\n",
      "| word_count_subset |     old      |   1   | 0.08539618866781597  |\n",
      "| word_count_subset |     car      |   1   | 0.05883499006802043  |\n",
      "| word_count_subset |     less     |   1   | -0.20970981521595625 |\n",
      "| word_count_subset |   product    |   1   |  -0.320555492995575  |\n",
      "| word_count_subset |    would     |   1   | -0.36230894771100114 |\n",
      "| word_count_subset |     even     |   1   |  -0.511738551270056  |\n",
      "| word_count_subset |     work     |   1   | -0.6217000124253141  |\n",
      "| word_count_subset |    money     |   1   | -0.8978841557762813  |\n",
      "| word_count_subset |    broke     |   1   |  -1.657964478380277  |\n",
      "| word_count_subset |    waste     |   1   | -2.0427736110037236  |\n",
      "| word_count_subset |    return    |   1   | -2.1172965971846347  |\n",
      "| word_count_subset | disappointed |   1   | -2.3550925006107253  |\n",
      "+-------------------+--------------+-------+----------------------+\n",
      "+----------------------+\n",
      "|        stderr        |\n",
      "+----------------------+\n",
      "| 0.04823282753835013  |\n",
      "| 0.04986195229399483  |\n",
      "| 0.03035462951090515  |\n",
      "| 0.012088854133053467 |\n",
      "| 0.02928886920202952  |\n",
      "|  0.0209509926590501  |\n",
      "| 0.021469147566490397 |\n",
      "| 0.02138130063099004  |\n",
      "|  0.0337581955697336  |\n",
      "| 0.020086342302457445 |\n",
      "| 0.01682915320908738  |\n",
      "|  0.0405057359539906  |\n",
      "| 0.015431132136201649 |\n",
      "| 0.012754475198474351 |\n",
      "| 0.019961276026100986 |\n",
      "| 0.023033059794584847 |\n",
      "|  0.0339936732835739  |\n",
      "| 0.05808789071659969  |\n",
      "| 0.06447029324442458  |\n",
      "| 0.05786508072405473  |\n",
      "| 0.05041498885569792  |\n",
      "+----------------------+\n",
      "[21 rows x 5 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']\n",
    "\n",
    "train_data['word_count_subset'] = train_data['word_count'].dict_trim_by_keys(significant_words, exclude=False)\n",
    "test_data['word_count_subset'] = test_data['word_count'].dict_trim_by_keys(significant_words, exclude=False)\n",
    "\n",
    "simple_model = tc.logistic_classifier.create(train_data,\n",
    "                                                     target = 'sentiment',\n",
    "                                                     features=['word_count_subset'],\n",
    "                                                     validation_set=None, verbose = False)\n",
    "\n",
    "acc2 = get_classification_accuracy(simple_model, test_data, test_data['sentiment'])\n",
    "\n",
    "simple_model.coefficients.sort('value', ascending=False).print_rows(num_rows=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare simple model and sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2train = get_classification_accuracy(simple_model, train_data, train_data['sentiment'])\n",
    "acc1train = get_classification_accuracy(sentiment_model, train_data, train_data['sentiment'])\n",
    "acc2train>acc1train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668150746537147 0.976494573364514\n"
     ]
    }
   ],
   "source": [
    "print(acc2train,acc1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112164\n",
      "21252\n"
     ]
    }
   ],
   "source": [
    "num_positive  = (train_data['sentiment'] == +1).sum()\n",
    "num_negative = (train_data['sentiment'] == -1).sum()\n",
    "print(num_positive)\n",
    "print(num_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8693004559635229 0.9221862251019919\n"
     ]
    }
   ],
   "source": [
    "print(acc2,acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
